{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff931b70-10ed-41f3-aa5f-afa93acc352a",
   "metadata": {},
   "source": [
    "# Cosine Distance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a60a37-eda8-4da2-aff7-f485cf627e50",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview\n",
    "The notebook is designed to process and analyze a collection of papers represented by their embedding vector data over a series of years. It calculates the cosine similarities between the vectors of papers for a given year and those from the previous five years, aiming to measure the similarity in content. The results, including the average and maximum cosine similarities for each paper, are then saved to a CSV file for further analysis or reference.\n",
    "The code is optimized to run it on the GPU if available.\n",
    "\n",
    "## Workflow\n",
    "- **Loading Data**: The notebook starts by loading vector data of papers for a specific year from a designated directory. Each row in the data files corresponds to a paper, with one column representing the paper's ID and the remaining columns representing the vector.\n",
    "\n",
    "> **_NOTE:_**  All vectors are assumed to be stored in csv files divided by years.\n",
    "\n",
    "- **Data Segmentation**: To optimize memory usage, the data is divided into manageable chunks. This segmentation facilitates efficient processing, especially for large datasets.\n",
    "\n",
    "- **Rolling Data Collection**: The vector data for the current year is added to a rolling collection that holds the data for the current and previous five years. This rolling mechanism ensures that only the most relevant five years of data are considered at any given time.\n",
    "\n",
    "- **Cosine Similarity Calculation**: If there are at least five years of data in the rolling collection, the notebook proceeds to calculate the cosine similarities. It compares the vectors of the current year’s papers with the combined vectors of the papers from the previous five years.\n",
    "\n",
    "- **Average and Maximum Similarities**: For each paper in the current year, both the average and maximum cosine similarities are calculated in relation to the papers from the previous years.\n",
    "\n",
    "- **Result Storage**: The calculated average and maximum cosine similarities, along with the paper IDs, are saved to a CSV file.\n",
    "\n",
    "- **Iteration**: The notebook repeats this process for each year in the specified range, ensuring that each year’s data is compared with the data from its preceding years.\n",
    "\n",
    "## Note\n",
    "\n",
    "- Ensure `cupy` is installed to run on GPU, you can install it via `pip install cupy`.\n",
    "- Adjust the CHUNK_SIZE based on GPU memory availability if running on GPU.\n",
    "- This notebook assumes that vectors are stored in consecutive years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0901020c-150c-407c-8ff5-9d9777e85096",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "\n",
    "# Try to import cupy for GPU acceleration, fall back to numpy if not available\n",
    "try:\n",
    "    import cupy as xp\n",
    "    print(\"Running on GPU\")\n",
    "except ImportError:\n",
    "    import numpy as xp\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "# Constants\n",
    "path_vectors = '../data/vectors/'\n",
    "path_vectors = 'D:/Users/Nicola Melluso/Work/MAG/NBER/data/papers_vectors/'\n",
    "CHUNK_SIZE = 1000  # Adjust based on memory availability\n",
    "OUTPUT_PATH = '../data/metrics/papers_cosine.csv'  # Adjust this path as needed\n",
    "N_JOBS = -1  # Use all available cores\n",
    "\n",
    "def load_vectors_for_year(year):\n",
    "    \"\"\"Load vectors for a specific year using efficient reading.\"\"\"\n",
    "    print(f'Reading {year}...')\n",
    "    \n",
    "    file_path = os.path.join(path_vectors, f\"{year}_vectors.csv\")\n",
    "\n",
    "    # Load the entire CSV into a single numpy array\n",
    "    data = xp.loadtxt(file_path, delimiter='\\t', dtype=np.float32)\n",
    "\n",
    "    # Slice the array to get the desired columns\n",
    "    papers_ids = data[:, 0].astype(xp.int64)  # Assuming the first column is the PaperId\n",
    "    vectors = data[:, 1:]  # Assuming the rest of the columns are the vectors\n",
    "\n",
    "    return papers_ids, vectors\n",
    "\n",
    "def cosine_similarity(vector_a, vector_b):\n",
    "    \"\"\"Simple cosine similarity function\"\"\"\n",
    "    \n",
    "    norm_a = xp.linalg.norm(vector_a)\n",
    "    norm_b = xp.linalg.norm(vector_b)\n",
    "    \n",
    "    dot_product = xp.dot(vector_a, vector_b)\n",
    "    \n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "def calculate_similarity_for_chunk(chunk, prior_data):\n",
    "    \"\"\"Calculate similarity for a chunk using matrix multiplication.\"\"\"\n",
    "    # Normalize the vectors\n",
    "    chunk_norm = chunk / xp.linalg.norm(chunk, axis=1, keepdims=True)\n",
    "    prior_data_norm = prior_data / xp.linalg.norm(prior_data, axis=1, keepdims=True)\n",
    "    \n",
    "    # Compute cosine similarities using matrix multiplication\n",
    "    similarities = xp.dot(chunk_norm, prior_data_norm.T)\n",
    "    \n",
    "    avg_dists = xp.mean(similarities, axis=1)\n",
    "    max_dists = xp.max(similarities, axis=1)\n",
    "    \n",
    "    return avg_dists, max_dists\n",
    "\n",
    "def calculate_avg_max_similarity(current_data, prior_data):\n",
    "    \"\"\"Calculate average and max cosine similarities for chunks.\"\"\"\n",
    "    results = Parallel(n_jobs=N_JOBS)(\n",
    "        delayed(calculate_similarity_for_chunk)(current_data[i:i+CHUNK_SIZE], prior_data)\n",
    "        for i in tqdm(range(0, len(current_data), CHUNK_SIZE))\n",
    "    )\n",
    "    avg_similarities = xp.concatenate([res[0] for res in results])\n",
    "    max_similarities = xp.concatenate([res[1] for res in results])\n",
    "    return avg_similarities, max_similarities\n",
    "\n",
    "def initialize_output_file():\n",
    "    \"\"\"Initialize the output CSV file with headers.\"\"\"\n",
    "    with open(OUTPUT_PATH, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['PaperId', 'cosine_max', 'cosine_avg'])\n",
    "\n",
    "def save_to_csv(papers_ids, avg_similarities, max_similarities):\n",
    "    \"\"\"Append results to CSV.\"\"\"\n",
    "    with open(OUTPUT_PATH, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for paper_id, avg_sim, max_sim in zip(papers_ids, avg_similarities, max_similarities):\n",
    "            writer.writerow([paper_id, max_sim, avg_sim])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc012c4-9ca6-4e6c-ad4c-ea62cb497b18",
   "metadata": {},
   "source": [
    "### Define the years "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4043583-a842-4545-a7c3-cede7710d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 1896\n",
    "end_year = 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b13f5-20d8-42e9-8dc4-612007d2c6be",
   "metadata": {},
   "source": [
    "### Run the similarity calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e3ff77d-c192-462b-8720-0fc12f5e89da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af67a475cc124adcbfdee3b2d5bfb435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1896...\n",
      "Reading 1897...\n",
      "Reading 1898...\n",
      "Reading 1899...\n",
      "Reading 1900...\n",
      "Reading 1901...\n",
      "Calculating similarities for 1901...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e618ea6c4fb747be8dff10d19037d32c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1902...\n",
      "Calculating similarities for 1902...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd9ffc8423b4c56835d4defb97d0e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1903...\n",
      "Calculating similarities for 1903...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91d8c6a1798454aa8c7396832dd6685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1904...\n",
      "Calculating similarities for 1904...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13cd0dbda0d94e56847e5ea5f7b53d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1905...\n",
      "Calculating similarities for 1905...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c7c571b8ca4e18aa4506706eedf5c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1906...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\9/ipykernel_28560/1497169036.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0myear\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myears\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mpapers_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_year_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_vectors_for_year\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Add current year data to rolling data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\9/ipykernel_28560/1430998006.py\u001b[0m in \u001b[0;36mload_vectors_for_year\u001b[1;34m(year)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# Load the entire CSV into a single numpy array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# Slice the array to get the desired columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[0;32m   1371\u001b[0m         \u001b[0mdelimiter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1373\u001b[1;33m     arr = _read(fname, dtype=dtype, comment=comment, delimiter=delimiter,\n\u001b[0m\u001b[0;32m   1374\u001b[0m                 \u001b[0mconverters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconverters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskiplines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskiprows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1375\u001b[0m                 \u001b[0munpack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munpack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mndmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[0;32m   1014\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mread_dtype_via_object_chunks\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1016\u001b[1;33m             arr = _load_from_filelike(\n\u001b[0m\u001b[0;32m   1017\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomment\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquote\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m                 \u001b[0mimaginary_unit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimaginary_unit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rolling_data = []\n",
    "years = range(start_year, end_year + 1) # +1 to include the last year\n",
    "\n",
    "# Initialize the output CSV file\n",
    "initialize_output_file()\n",
    "\n",
    "for year in tqdm(years):\n",
    "    papers_ids, current_year_data = load_vectors_for_year(year)\n",
    "\n",
    "    # Add current year data to rolling data\n",
    "    rolling_data.append((year, current_year_data))\n",
    "    \n",
    "    # Remove data that is more than 5 years old\n",
    "    rolling_data = [(y, data) for y, data in rolling_data if year - y < 6]\n",
    "\n",
    "    # If there's not enough prior data, skip the calculations for this year\n",
    "    if len(rolling_data) < 6:\n",
    "        continue\n",
    "\n",
    "    # Combine prior years data\n",
    "    prior_data = xp.vstack([data for y, data in rolling_data if y != year])\n",
    "    \n",
    "    print('Calculating similarities for %d...'%(year))\n",
    "    # Calculate cosine similarities\n",
    "    avg_year_similarities, max_year_similarities = calculate_avg_max_similarity(current_year_data, prior_data)\n",
    "\n",
    "    # Save results to CSV\n",
    "    save_to_csv(papers_ids, avg_year_similarities, max_year_similarities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
