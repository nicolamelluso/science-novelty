{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff931b70-10ed-41f3-aa5f-afa93acc352a",
   "metadata": {},
   "source": [
    "# Cosine Distance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a60a37-eda8-4da2-aff7-f485cf627e50",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview\n",
    "The notebook is designed to process and analyze a collection of papers represented by their embedding vector data over a series of years. It calculates the cosine similarities between the vectors of papers for a given year and those from the previous five years, aiming to measure the similarity in content. The results, including the average and maximum cosine similarities for each paper, are then saved to a CSV file for further analysis or reference.\n",
    "The code is optimized to run it on the GPU if available.\n",
    "\n",
    "## Improvements\n",
    "\n",
    "For an efficient handling of the embeddings, we suggest using optimized embedding database structures. [ChromaDB](https://www.trychroma.com/) is a valid open-source application database that works fast and efficiently with embeddings. Also BigQuery is recently providing services to store and query embedding data\n",
    "\n",
    "## Workflow\n",
    "- **Loading Data**: The notebook starts by loading vector data of papers for a specific year from a designated directory. Each row in the data files corresponds to a paper, with one column representing the paper's ID and the remaining columns representing the vector.\n",
    "\n",
    "> **_NOTE:_**  All vectors are assumed to be stored in csv files divided by years.\n",
    "\n",
    "- **Data Segmentation**: To optimize memory usage, the data is divided into manageable chunks. This segmentation facilitates efficient processing, especially for large datasets.\n",
    "\n",
    "- **Rolling Data Collection**: The vector data for the current year is added to a rolling collection that holds the data for the current and previous five years. This rolling mechanism ensures that only the most relevant five years of data are considered at any given time.\n",
    "\n",
    "- **Cosine Similarity Calculation**: If there are at least six years of data in the rolling collection, the notebook proceeds to calculate the cosine similarities. It compares the vectors of the current year’s papers with the combined vectors of the papers from the previous five years.\n",
    "\n",
    "- **Average and Maximum Similarities**: For each paper in the current year, both the average and maximum cosine similarities are calculated in relation to the papers from the previous years.\n",
    "\n",
    "- **Result Storage**: The calculated average and maximum cosine similarities, along with the paper IDs, are saved to a CSV file.\n",
    "\n",
    "- **Iteration**: The notebook repeats this process for each year in the specified range, ensuring that each year’s data is compared with the data from its preceding years.\n",
    "\n",
    "## Note\n",
    "\n",
    "- Ensure `cupy` is installed to run on GPU, you can install it via `pip install cupy`.\n",
    "- Adjust the CHUNK_SIZE based on GPU memory availability if running on GPU.\n",
    "- This notebook assumes that vectors are stored in consecutive years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0901020c-150c-407c-8ff5-9d9777e85096",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "\n",
    "# Try to import cupy for GPU acceleration, fall back to numpy if not available\n",
    "try:\n",
    "    import cupy as xp\n",
    "    print(\"Running on GPU\")\n",
    "except ImportError:\n",
    "    import numpy as xp\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "# Constants\n",
    "path_vectors = '../data/vectors/'\n",
    "CHUNK_SIZE = 1000  # Adjust based on memory availability\n",
    "OUTPUT_PATH = '../data/metrics/papers_cosine.csv'  # Adjust this path as needed\n",
    "N_JOBS = -1  # Use all available cores\n",
    "\n",
    "def load_vectors_for_year(year):\n",
    "    \"\"\"Load vectors for a specific year using efficient reading.\"\"\"\n",
    "    \n",
    "    file_path = os.path.join(path_vectors, f\"{year}_vectors.csv\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        return None, None\n",
    "    \n",
    "    print(f'Reading {year}...')\n",
    "    # Load the entire CSV into a single numpy array\n",
    "    data = xp.loadtxt(file_path, delimiter=',', dtype=np.float32, skiprows = 1)\n",
    "    \n",
    "    # Check if there is only one paper in the year\n",
    "    if len(data) == 769:\n",
    "        papers_ids = [data[0].astype(xp.int64)]\n",
    "        vectors = [data[1:]]\n",
    "        \n",
    "    else:\n",
    "        # Slice the array to get the desired columns\n",
    "        papers_ids = data[:, 0].astype(xp.int64)  # Assuming the first column is the PaperId\n",
    "        vectors = data[:, 1:]  # Assuming the rest of the columns are the vectors\n",
    "\n",
    "    return papers_ids, vectors\n",
    "\n",
    "def cosine_similarity(vector_a, vector_b):\n",
    "    \"\"\"Simple cosine similarity function\"\"\"\n",
    "    \n",
    "    norm_a = xp.linalg.norm(vector_a)\n",
    "    norm_b = xp.linalg.norm(vector_b)\n",
    "    \n",
    "    dot_product = xp.dot(vector_a, vector_b)\n",
    "    \n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "def calculate_similarity_for_chunk(chunk, prior_data):\n",
    "    \"\"\"Calculate similarity for a chunk using matrix multiplication.\"\"\"\n",
    "    # Normalize the vectors\n",
    "    chunk_norm = chunk / xp.linalg.norm(chunk, axis=1, keepdims=True)\n",
    "    prior_data_norm = prior_data / xp.linalg.norm(prior_data, axis=1, keepdims=True)\n",
    "    \n",
    "    # Compute cosine similarities using matrix multiplication\n",
    "    similarities = xp.dot(chunk_norm, prior_data_norm.T)\n",
    "    \n",
    "    avg_dists = xp.mean(similarities, axis=1)\n",
    "    max_dists = xp.max(similarities, axis=1)\n",
    "    \n",
    "    return avg_dists, max_dists\n",
    "\n",
    "def calculate_avg_max_similarity(current_data, prior_data):\n",
    "    \"\"\"Calculate average and max cosine similarities for chunks.\"\"\"\n",
    "    results = Parallel(n_jobs=N_JOBS)(\n",
    "        delayed(calculate_similarity_for_chunk)(current_data[i:i+CHUNK_SIZE], prior_data)\n",
    "        for i in tqdm(range(0, len(current_data), CHUNK_SIZE))\n",
    "    )\n",
    "    avg_similarities = xp.concatenate([res[0] for res in results])\n",
    "    max_similarities = xp.concatenate([res[1] for res in results])\n",
    "    return avg_similarities, max_similarities\n",
    "\n",
    "def initialize_output_file():\n",
    "    \"\"\"Initialize the output CSV file with headers.\"\"\"\n",
    "    with open(OUTPUT_PATH, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['PaperId', 'cosine_max', 'cosine_avg'])\n",
    "\n",
    "def save_to_csv(papers_ids, avg_similarities, max_similarities):\n",
    "    \"\"\"Append results to CSV.\"\"\"\n",
    "    with open(OUTPUT_PATH, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for paper_id, avg_sim, max_sim in zip(papers_ids, avg_similarities, max_similarities):\n",
    "            writer.writerow([paper_id, max_sim, avg_sim])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc012c4-9ca6-4e6c-ad4c-ea62cb497b18",
   "metadata": {},
   "source": [
    "### Define the years "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4043583-a842-4545-a7c3-cede7710d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 1896\n",
    "end_year = 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b13f5-20d8-42e9-8dc4-612007d2c6be",
   "metadata": {},
   "source": [
    "### Run the similarity calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3ff77d-c192-462b-8720-0fc12f5e89da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rolling_data = []\n",
    "years = range(start_year, end_year + 1) # +1 to include the last year\n",
    "\n",
    "# Initialize the output CSV file\n",
    "initialize_output_file()\n",
    "\n",
    "for year in tqdm(years):\n",
    "    papers_ids, current_year_data = load_vectors_for_year(year)\n",
    "    \n",
    "    if current_year_data is None:\n",
    "        continue\n",
    "\n",
    "    # Add current year data to rolling data\n",
    "    rolling_data.append((year, current_year_data))\n",
    "    \n",
    "    # Remove data that is more than 5 years old\n",
    "    rolling_data = [(y, data) for y, data in rolling_data if year - y < 6]\n",
    "\n",
    "    # If there's not enough prior data, skip the calculations for this year\n",
    "    if len(rolling_data) < 6:\n",
    "        continue\n",
    "\n",
    "    # Combine prior years data\n",
    "    prior_data = xp.vstack([data for y, data in rolling_data if y != year])\n",
    "    \n",
    "    print('Calculating similarities for %d...'%(year))\n",
    "    # Calculate cosine similarities\n",
    "    avg_year_similarities, max_year_similarities = calculate_avg_max_similarity(current_year_data, prior_data)\n",
    "\n",
    "    # Save results to CSV\n",
    "    save_to_csv(papers_ids, avg_year_similarities, max_year_similarities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
