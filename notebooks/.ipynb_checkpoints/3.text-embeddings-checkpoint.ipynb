{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac34b70-d698-4791-bea2-e6c5302f9962",
   "metadata": {},
   "source": [
    "# Text Embeddings with SPECTER\n",
    "\n",
    "## Overview \n",
    "\n",
    "This Python script is dedicated to generating and storing embeddings for a set of papers. It reads the raw data, processes the text of titles and abstracts, and converts them into embeddings using a pre-loaded model. In this case, the embedding vectors is a 768-dimensional vector and the model is the SPECTER. The embeddings are then stored in either CSV or NumPy format, organized by the publication year of the papers.\n",
    "\n",
    "Transforming raw text into embeddings is a computationally intensive task. While it's possible to perform this operation on a CPU, it's not recommended due to the significant time and resource overhead. Instead, using a GPU is advisable for efficiency.\n",
    "\n",
    "Given the computational demands of this task, it's essential to adopt a memory-efficient approach. Instead of loading the entire dataset into memory, we'll process the data iteratively. This involves reading each paper's text line-by-line, generating its embedding, and then immediately writing the embedding to storage. This method ensures that only a minimal amount of data is held in memory at any given time.\n",
    "\n",
    "## Workflow\n",
    "\n",
    "- **Loading the Embedding Model**:The notebook starts by loading a pre-trained embedding model. If a GPU is available, the model is moved to the GPU to accelerate the processing; otherwise, it continues using the CPU.\n",
    "\n",
    "- **Counting the Number of Papers**: It counts the total number of papers in the dataset by reading the raw CSV file and counting the lines. This is required to keep track of the progress of the process with a progress bar (tqdm)\n",
    "\n",
    "- **Setting Storage Method:** Users can choose between 'csv' and 'numpy' as the storage method for the generated embeddings. This choice determines the format in which the embeddings will be saved.\n",
    "\n",
    "- **Processing Data in Chunks:** To optimize memory usage, the script processes the data in chunks. It reads and processes a specified number of papers at a time, generating embeddings for each chunk before moving on to the next.\n",
    "\n",
    "- **Generating and Storing Embeddings:** For each chunk of data, the script performs the following steps:\n",
    "    - Reads the papers’ data from the raw CSV file.\n",
    "    - Extracts the publication year of each paper.\n",
    "    - Combines the title and abstract of each paper and generates an embedding using the pre-loaded model.\n",
    "    - Stores the generated embeddings in a list, organized by publication year.\n",
    "\n",
    "\n",
    "- **Saving Embeddings:** When all papers for a specific year have been processed, or when moving to papers from a new year, the notebook saves the embeddings to disk. It creates a new file for each year, storing the embeddings in either CSV or NumPy format depending on the chosen storage method.\n",
    "\n",
    "\n",
    "## Considerations for Storing Embeddings\n",
    "\n",
    "When deciding how to store the generated embeddings, several factors come into play:\n",
    "\n",
    "- **Read/Write Speed**: For operations where speed is crucial, binary formats like numpy's `.npy` or `.npz` (for sparse matrices) are recommended. These formats offer faster read/write speeds compared to traditional CSV files.\n",
    "\n",
    "- **Interoperability**: If the embeddings need to be accessed by various software or tools, the CSV format is more universal. However, it's worth noting that CSV files tend to be larger and slower to read/write compared to binary formats.\n",
    "\n",
    "- **Data Volume**: If dealing with a vast amount of embeddings, it might be beneficial to process and store the data in chunks. This approach can further optimize memory usage and improve overall efficiency.\n",
    "\n",
    "With these considerations in mind, we'll now delve into the process of generating and storing embeddings using the SPECTER model.\n",
    "\n",
    "## Output\n",
    "The notebook generates files containing the embeddings of the papers, organized by their publication year. Each file is named after the corresponding year and contains the embeddings in either CSV or NumPy format, depending on the user’s choice.\n",
    "\n",
    "\n",
    "## -- Note\n",
    "To generate the embeddings used to calculate the semantic distance in the [Zenodo repository](https://zenodo.org/records/13869486), we used the [SPECTER public API](https://github.com/allenai/paper-embedding-public-apis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd18cb1-73dd-4dbf-a3db-fafccdcff64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../science_novelty/')\n",
    "\n",
    "from embeddings import load_model, get_embedding\n",
    "\n",
    "## Increase the max size of a line reading, otherwise an error is raised\n",
    "maxInt = sys.maxsize\n",
    "\n",
    "while True:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10dc04c-89d3-4331-b478-038c8ba43ae4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "PATH_OUTPUT = '../data/'\n",
    "PATH_INPUT = '../data/raw/'\n",
    "STORAGE = 'csv'\n",
    "CHUNK_SIZE = 50\n",
    "TOTAL_PAPERS = None\n",
    "\n",
    "# Check if paths exist\n",
    "if not os.path.exists(PATH_OUTPUT) or not os.path.exists(PATH_INPUT):\n",
    "    raise Exception(\"Input or output path does not exist.\")\n",
    "\n",
    "# Load the embedding model\n",
    "print('Loading the embedding model...')\n",
    "tokenizer, model = load_model()\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "print(f\"Using {device.upper()}.\")\n",
    "\n",
    "# Count the number of papers\n",
    "print('Get the number of papers to process...')\n",
    "with open(PATH_INPUT + 'papers_raw.csv', 'r', encoding='utf-8') as file:\n",
    "    line_count = sum(1 for line in file)\n",
    "TOTAL_PAPERS = line_count - 1  # Subtract 1 for the header\n",
    "\n",
    "\n",
    "def get_last_processed_index(path_output):\n",
    "    total_processed = 0\n",
    "    vectors_path = os.path.join(path_output, 'vectors')\n",
    "    \n",
    "    if not os.path.exists(vectors_path):\n",
    "        return total_processed\n",
    "    \n",
    "    for file in os.listdir(vectors_path):\n",
    "        if file.endswith('.csv') or file.endswith('.npy'):\n",
    "            file_path = os.path.join(vectors_path, file)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                total_processed += sum(1 for line in f) - 1  # Subtract 1 to exclude the header row\n",
    "    \n",
    "    return total_processed\n",
    "\n",
    "def save_vectors(vectors, year, storage, path_output):\n",
    "    vectors_path = os.path.join(path_output, 'vectors')\n",
    "    os.makedirs(vectors_path, exist_ok=True)  # Ensure the directory exists\n",
    "    \n",
    "    file_path = os.path.join(vectors_path, f'{year}_vectors')\n",
    "    \n",
    "    if storage == 'csv':\n",
    "        file_path += '.csv'\n",
    "        mode = 'a' if os.path.exists(file_path) else 'w'\n",
    "        with open(file_path, mode, encoding='utf-8', newline='') as writer:\n",
    "            csv_writer = csv.writer(writer, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            if mode == 'w':\n",
    "                print(f'Creating new file for year {year}...')\n",
    "                csv_writer.writerow([\"PaperID\"] + [f\"{i}\" for i in range(len(vectors[0]) - 1)])  # Adjusted header format\n",
    "            csv_writer.writerows(vectors)\n",
    "    elif storage == 'numpy':\n",
    "        file_path += '.npy'\n",
    "        vectors = np.array([vec[1:] for vec in vectors])  # Exclude PaperID for numpy storage\n",
    "        if os.path.exists(file_path):\n",
    "            existing_vectors = np.load(file_path, allow_pickle=True)\n",
    "            vectors = np.vstack((existing_vectors, vectors))\n",
    "        np.save(file_path, vectors)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported storage format. Use 'csv' or 'numpy'.\")\n",
    "\n",
    "def process_papers(start_index):\n",
    "    with open(PATH_INPUT + 'papers_raw.csv', 'r', encoding='utf-8') as reader:\n",
    "        csv_reader = csv.reader(reader, delimiter='\\t', quotechar='\"')\n",
    "\n",
    "        # Skip headers and already processed papers\n",
    "        print('Already done papers...')\n",
    "        for _ in tqdm(range(start_index + 2)):\n",
    "            next(csv_reader)\n",
    "\n",
    "        for chunk_start in tqdm(range(start_index, TOTAL_PAPERS, CHUNK_SIZE)):\n",
    "            chunk_data = [line_csv for _, line_csv in zip(range(CHUNK_SIZE), csv_reader)]\n",
    "            \n",
    "            # Group by year\n",
    "            papers_by_year = {}\n",
    "            for data in chunk_data:\n",
    "                year = int(data[1].split('-')[0])\n",
    "                if year not in papers_by_year:\n",
    "                    papers_by_year[year] = []\n",
    "                papers_by_year[year].append(data)\n",
    "\n",
    "            # Process each year group\n",
    "            for year, papers in papers_by_year.items():\n",
    "                texts = [paper[2] + paper[3] for paper in papers]\n",
    "                vectors = get_embedding(texts, tokenizer, model)\n",
    "                vectors_with_id = [[paper[0]] + list(vectors[i]) for i, paper in enumerate(papers)]\n",
    "                save_vectors(vectors_with_id, year, STORAGE, PATH_OUTPUT)\n",
    "\n",
    "\n",
    "# Get the last processed paper index\n",
    "last_processed_index = get_last_processed_index(PATH_OUTPUT)\n",
    "print(f\"Resuming from paper {last_processed_index + 1}.\")\n",
    "\n",
    "# Process the papers\n",
    "process_papers(last_processed_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
