{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00a1b216-29bc-4289-a2c1-eace8fb2439f",
   "metadata": {},
   "source": [
    "# Overview\n",
    "The data structure of this repository is organized within a primary directory named `data`. This directory contains three subdirectories:\n",
    "\n",
    "- `/raw`: Contains the raw text data of papers. This can be sourced from OpenAlex, a custom dataset, or other sources.\n",
    "- `/processed`: Contains processed versions of the papers. Papers are sorted by publication date (according to MAG). Three specific files are expected here:\n",
    "    - `papers_words.csv`\n",
    "    - `papers_bigrams.csv`\n",
    "    - `papers_trigrams.csv`\n",
    "    \n",
    "    Each of the processed files should have the following three columns:\n",
    "    - *`PaperID`*\n",
    "    - *`Title_Words`* (or `Title_Bigrams` for bigrams, or `Title_Trigrams`)\n",
    "    - *`Abstract_Words`* (or `Abstract_Bigrams` for trigrams, or `Abstract_Trigrams`)\n",
    "    \n",
    "> **_NOTE 1:_**  All papers MUST be sorted by publication date.\n",
    "\n",
    "> **_NOTE 2:_** Nan (`np.nan`) are placed where the abstract is unavailable.\n",
    "\n",
    "\n",
    "# Data Selection Guide: *Choosing the Dataset*\n",
    "This repository is based on Microsoft Academic Graph (MAG) data. The data output is available at https://zenodo.org/record/8283353, that provides data on all MAG papers (both journal and conference papers) published between 1800 and 2020.\n",
    "\n",
    "While the underlying code remains consistent, users might have different data requirements. Some may prefer a custom dataset. This notebook outlines four distinct approaches to data acquisition.\n",
    "\n",
    "- **Approach 1**: Use custom data\n",
    "- **Approach 2**: Use OpenAlex data\n",
    "- **Approach 3**: Use a subset of Zenodo (MAG papers) data\n",
    "- **Approach 4**. Use the entire Zenodo (MAG papers) data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156b6abf-cfd8-49d2-a113-bc9dc240a40c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Approach 1: Custom Data\n",
    "Data should be placed in a file named `papers_raw.csv` within the `data/raw/` directory.\n",
    "- The file must have the following columns:\n",
    "    - *`PaperID`*: Unique identifier of the paper\n",
    "    - *`Date`*: Date of publication of the paper in the format *yyyy-mm-dd*\n",
    "    - *`Title`*: Title of the paper(non-processed)\n",
    "    - *`Abstract`*: Abstract of the paper (non-processed)\n",
    "    \n",
    "If an abstract is unavailable, use a placeholder value of None or `np.nan`.\n",
    "\n",
    "> **_NOTE:_**  All papers MUST be sorted by publication date."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17021090-ee0e-4f8c-aba9-c396b9c363df",
   "metadata": {},
   "source": [
    "## Approach 2: OpenAlex data\n",
    "\n",
    "Data from OpenAlex can be obtained using the OpenAlex API (https://docs.openalex.org/)\n",
    "\n",
    "There are several strategies to download data from OpenAlex. \n",
    "\n",
    "Here is shown the standard strategy of getting papers by searching for keywords in the title, abstract of full text. The following query is used:\n",
    "\n",
    "> **`(natural language processing) & novelty`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc9992c9-b8d7-44b7-8e51-dbac788f0eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading stopwords...\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\u0152835\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../scripts/')\n",
    "\n",
    "import preprocessing\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aad8086-3aa3-4423-b387-a144bfcae57a",
   "metadata": {},
   "source": [
    "### Query: Title, Abstract, Full text\n",
    "\n",
    "> **_NOTE:_**  All papers MUST be sorted by publication date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc00585d-893c-4ad0-b055-db45fe5b1b97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers: 3364\n",
      "Start querying...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcf2738653c4e3cb211a1614abc9ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the dataframe...\n",
      "Drop missing papers with missing title and abstract\n"
     ]
    }
   ],
   "source": [
    "# OpenAlex API URL\n",
    "url = \"https://api.openalex.org/works\"\n",
    "\n",
    "# This is an example query\n",
    "query = '(natural language processing) & novelty'\n",
    "\n",
    "# Define the initial page and per page variables\n",
    "page = 1\n",
    "per_page = 100 \n",
    "papers = []\n",
    "\n",
    "params = {'search': query, 'filter': 'type:article'}\n",
    "\n",
    "# Send a GET request to the API\n",
    "response = requests.get(url, params=params)\n",
    "count = response.json()['meta']['count']\n",
    "total_pages = round(count/per_page) + 1\n",
    "\n",
    "print('Total papers: %d'%(count))\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "print('Start querying...')\n",
    "# Loop through all pages (+1 to get the last page)\n",
    "for page in tqdm(range(1,total_pages + 1)):\n",
    "    \n",
    "    # Get the cursor for the first page\n",
    "    if page == 1:\n",
    "        cursor = '*'\n",
    "\n",
    "    params = {\n",
    "        'search': query,\n",
    "        'sort': 'publication_date',\n",
    "        'per-page': per_page,\n",
    "        'filter': 'type:article',\n",
    "        'cursor' : cursor\n",
    "    }\n",
    "\n",
    "    # Send a GET request to the API\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    # If the request is successful\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "\n",
    "        # Get the data\n",
    "        results = data.get('results',[])\n",
    "        \n",
    "        # Select the information need from these publications\n",
    "        papers.extend([((res['id'].split('/')[-1].replace('W','')),\n",
    "                        res['publication_date'],\n",
    "                        res['title'],\n",
    "                        preprocessing.plain_text_from_inverted(res['abstract_inverted_index'])) \n",
    "                               for res in results])\n",
    "        \n",
    "        # Get the next cursor for the pagination\n",
    "        cursor = response.json()['meta']['next_cursor']\n",
    "\n",
    "        # Respect the API rate limit\n",
    "        time.sleep(1)\n",
    "        \n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}.\")\n",
    "        break\n",
    "    \n",
    "print('Creating the dataframe...')\n",
    "papers = pd.DataFrame(papers, columns = ['PaperID','Date','Title','Abstract'])\n",
    "\n",
    "print('Drop missing papers with missing title and abstract')\n",
    "papers = papers.dropna(subset = ['Title','Abstract'], how = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588404db-9247-4304-bce3-c408c300f5e5",
   "metadata": {},
   "source": [
    "#### Export the data\n",
    "Make sure to export the data separated by tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddb8bef3-9504-4cdf-928d-9f1504d9f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers.to_csv('../data/raw/papers_raw.csv', index = False, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d55ca3-ffe3-42d2-985e-ae43dd50dc65",
   "metadata": {},
   "source": [
    "## Approach 3: Subset of Zenodo (MAG) Data\n",
    "The Zenodo repository, available at https://zenodo.org/record/8283353, provides data on all MAG papers (both journal and conference papers) published between 1800 and 2020. However, the raw text of titles and abstracts is not available. Instead, the repository offers processed versions of these papers.\n",
    "\n",
    "To extract a specific subset from the Zenodo repository, there are two recommended methods:\n",
    "\n",
    "1. **Textual Search**: Navigate through the processed titles and abstracts to identify desired papers.\n",
    "2. **ID-Based Selection**: Utilize a custom list of identifiers to filter desired papers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf39108-829b-42d0-a1cd-f057e4b38d3e",
   "metadata": {},
   "source": [
    "### Textual Search in the title and abstract \n",
    "\n",
    "In order to avoid overloading of the RAM, a line-by-line reading and writing is recommended.\n",
    "\n",
    "- Data from zenodo should be downloaded and placed in a separate external folder out of this repository\n",
    "- The file `papers_words.csv` will be read line-by-line\n",
    "- For each line the text to be queried will be searched and title and abstract containing it will be filter out in two files:\n",
    "    - `papers_ids.csv` placed in the `data/raw/` with one column and for each row the id of the paper containing the query\n",
    "    - `papers_words.csv`,`papers_bigrams.csv`,`papers_trigrams.csv`, placed in the `data/processed/` directory\n",
    "\n",
    "In this case, the following query strategy will be followed: search for the papers containing both `(natural language processing)` and `novelty`.\n",
    "\n",
    "Users can use their own searching strategies.\n",
    "\n",
    "> **_NOTE:_**  All papers are sorted by publication date (using MAG date)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c79c3f85-e433-4a69-a6e5-5667e13682b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing for reading...\n",
      "Preparing for writing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc423559b8b40f28abffa6665efc216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83490800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# select the words\n",
    "# all lower cased\n",
    "terms = ['natural language','novelty']\n",
    "\n",
    "path_external = 'D:/PublicDatabases/ScientificPublications/Papers/TitleAbstract/MAG_SN/Science_Novelty/'\n",
    "\n",
    "print('Preparing for reading...')\n",
    "words = open(path_external + 'papers_words.csv','r', encoding = 'utf-8')\n",
    "words.__next__()\n",
    "bigrams = open(path_external + 'papers_bigrams.csv','r', encoding = 'utf-8')\n",
    "bigrams.__next__()\n",
    "trigrams = open(path_external + 'papers_trigrams.csv','r', encoding = 'utf-8')\n",
    "trigrams.__next__()\n",
    "\n",
    "\n",
    "print('Preparing for writing...')\n",
    "words_write = open('../data/processed/papers_words.csv','w')\n",
    "words_write.write('PaperID,Words_Title,Words_Abstract\\n') # write the first line for the headers\n",
    "bigrams_write = open('../data/processed/papers_bigrams.csv','w')\n",
    "bigrams_write.write('PaperID,Bigrams_Title,Bigrams_Abstract\\n') # write the first line for the headers\n",
    "trigrams_write = open('../data/processed/papers_trigrams.csv','w')\n",
    "trigrams_write.write('PaperID,Trigrams_Title,Trigrams_Abstract\\n') # write the first line for the headers\n",
    "\n",
    "\n",
    "for line_words, line_bigrams, line_trigrams in tqdm(zip(words, bigrams, trigrams), total = 83490800):\n",
    "    \n",
    "    if all(term in line_words for term in terms):\n",
    "        \n",
    "        words_write.write(line_words)\n",
    "        bigrams_write.write(line_bigrams)\n",
    "        trigrams_write.write(line_trigrams)\n",
    "        \n",
    "print('Done.') \n",
    "\n",
    "words_write.close()\n",
    "bigrams_write.close()\n",
    "trigrams_write.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b97835e-5455-4c36-9429-01d1ba4357bd",
   "metadata": {},
   "source": [
    "### Use a custom set of papers\n",
    "To use a subset of the Zenodo data:\n",
    "\n",
    "1. Prepare a file named `papers_ids.csv` in the /raw directory.\n",
    "\n",
    "    This file should contain a single column with a header indicating the type of identifier used. Acceptable headers are:\n",
    "\n",
    "    - `PaperID` (for MAG or OpenAlex identifiers)\n",
    "    - `DOI` (for Digital Object Identifier)\n",
    "    - `PubMedID` (for PubMed's unique identifier)\n",
    "    \n",
    "    > Note: MAG and OpenAlex identifiers are similar. To convert a MAG identifier to a PubMed identifier, append 'W' to the MAG identifier.\n",
    "    \n",
    "2. Place the file with papers identifiers (`papers.csv` from the Zenodo repository) and the three original files of processed papers (`papers_words.csv`, `papers_bigrams.csv`, `papers_trigrams.csv`) in an external folder out of this repository\n",
    "\n",
    "3. Filter the files using the `papers_ids.csv`, creating three versions of the processed files (`papers_words.csv`, `papers_bigrams.csv`, `papers_trigrams.csv`) in the `data/processed/` directory. Ensure that the files contain only the data intended for the analysis.\n",
    "\n",
    "> **_NOTE:_**  All papers MUST be sorted by publication date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9a1483-59b5-494b-8671-ca56f62534e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50032f53-84b2-4647-ac0f-2445eb3741a3",
   "metadata": {},
   "source": [
    "## Approach 4: Entire Zenodo Repository\n",
    "For those interested in using the complete dataset (comprising 83,490,800 papers) from the Zenodo repository, refer to the three processed files to be placed in the `data/processed/` directory:\n",
    "\n",
    "- `papers_words.csv`\n",
    "- `papers_bigrams.csv`\n",
    "- `papers_trigrams.csv`\n",
    "\n",
    "> **_NOTE:_**  All papers MUST be sorted by publication date."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
