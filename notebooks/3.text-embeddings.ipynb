{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac34b70-d698-4791-bea2-e6c5302f9962",
   "metadata": {},
   "source": [
    "# Text Embeddings with SPECTER\n",
    "\n",
    "## Overview \n",
    "\n",
    "This Python script is dedicated to generating and storing embeddings for a set of papers. It reads the raw data, processes the text of titles and abstracts, and converts them into embeddings using a pre-loaded model. In this case, the embedding vectors is a 768-dimensional vector and the model is the SPECTER. The embeddings are then stored in either CSV or NumPy format, organized by the publication year of the papers.\n",
    "\n",
    "Transforming raw text into embeddings is a computationally intensive task. While it's possible to perform this operation on a CPU, it's not recommended due to the significant time and resource overhead. Instead, using a GPU is advisable for efficiency.\n",
    "\n",
    "Given the computational demands of this task, it's essential to adopt a memory-efficient approach. Instead of loading the entire dataset into memory, we'll process the data iteratively. This involves reading each paper's text line-by-line, generating its embedding, and then immediately writing the embedding to storage. This method ensures that only a minimal amount of data is held in memory at any given time.\n",
    "\n",
    "## Workflow\n",
    "\n",
    "- **Loading the Embedding Model**:The notebook starts by loading a pre-trained embedding model. If a GPU is available, the model is moved to the GPU to accelerate the processing; otherwise, it continues using the CPU.\n",
    "\n",
    "- **Counting the Number of Papers**: It counts the total number of papers in the dataset by reading the raw CSV file and counting the lines. This is required to keep track of the progress of the process with a progress bar (tqdm)\n",
    "\n",
    "- **Setting Storage Method:** Users can choose between 'csv' and 'numpy' as the storage method for the generated embeddings. This choice determines the format in which the embeddings will be saved.\n",
    "\n",
    "- **Processing Data in Chunks:** To optimize memory usage, the script processes the data in chunks. It reads and processes a specified number of papers at a time, generating embeddings for each chunk before moving on to the next.\n",
    "\n",
    "- **Generating and Storing Embeddings:** For each chunk of data, the script performs the following steps:\n",
    "    - Reads the papers’ data from the raw CSV file.\n",
    "    - Extracts the publication year of each paper.\n",
    "    - Combines the title and abstract of each paper and generates an embedding using the pre-loaded model.\n",
    "    - Stores the generated embeddings in a list, organized by publication year.\n",
    "\n",
    "\n",
    "- **Saving Embeddings:** When all papers for a specific year have been processed, or when moving to papers from a new year, the notebook saves the embeddings to disk. It creates a new file for each year, storing the embeddings in either CSV or NumPy format depending on the chosen storage method.\n",
    "\n",
    "\n",
    "## Considerations for Storing Embeddings\n",
    "\n",
    "When deciding how to store the generated embeddings, several factors come into play:\n",
    "\n",
    "- **Read/Write Speed**: For operations where speed is crucial, binary formats like numpy's `.npy` or `.npz` (for sparse matrices) are recommended. These formats offer faster read/write speeds compared to traditional CSV files.\n",
    "\n",
    "- **Interoperability**: If the embeddings need to be accessed by various software or tools, the CSV format is more universal. However, it's worth noting that CSV files tend to be larger and slower to read/write compared to binary formats.\n",
    "\n",
    "- **Data Volume**: If dealing with a vast amount of embeddings, it might be beneficial to process and store the data in chunks. This approach can further optimize memory usage and improve overall efficiency.\n",
    "\n",
    "With these considerations in mind, we'll now delve into the process of generating and storing embeddings using the SPECTER model.\n",
    "\n",
    "## Output\n",
    "The notebook generates files containing the embeddings of the papers, organized by their publication year. Each file is named after the corresponding year and contains the embeddings in either CSV or NumPy format, depending on the user’s choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd18cb1-73dd-4dbf-a3db-fafccdcff64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../science_novelty/')\n",
    "\n",
    "import embeddings\n",
    "import csv\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "## Increase the max size of a line reading, otherwise an error is raised\n",
    "maxInt = sys.maxsize\n",
    "\n",
    "while True:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343ccf4d-dca2-4667-88f8-7d96a6aab2d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the embedding model\n",
    "print('Load the embedding model...')\n",
    "tokenizer, model = embeddings.load_model()\n",
    "\n",
    "# Move the model to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to('cuda')\n",
    "    print(\"Model moved to GPU.\")\n",
    "else:\n",
    "    print(\"Using CPU.\")\n",
    "\n",
    "# Count the number of papers\n",
    "print('Get the number of papers to process...')\n",
    "with open('../data/raw/papers_raw.csv', 'r', encoding='utf-8') as file:\n",
    "    line_count = sum(1 for line in file)\n",
    "total_papers = line_count - 1  # Subtract 1 for the header\n",
    "\n",
    "# Choose storage method: 'csv' or 'numpy'\n",
    "storage = 'csv'  # Change to 'numpy' if needed\n",
    "\n",
    "# Process data in chunks for memory efficiency\n",
    "chunk_size = 50\n",
    "print('Processing...')\n",
    "current_year_vectors = []\n",
    "current_year = None\n",
    "\n",
    "with open('../data/raw/papers_raw.csv', 'r', encoding='utf-8') as reader:\n",
    "    csv_reader = csv.reader(reader, delimiter='\\t', quotechar='\"')\n",
    "    next(csv_reader)  # Skip header\n",
    "\n",
    "    for chunk_start in tqdm(range(0, total_papers, chunk_size)):\n",
    "        chunk_data = [line for _, line in zip(range(chunk_size), csv_reader)]\n",
    "        \n",
    "        # Generate embeddings for the chunk\n",
    "        for line in chunk_data:\n",
    "            year = int(line[1].split('-')[0])  # Assuming the year is in the second column\n",
    "            \n",
    "            if current_year is None:\n",
    "                current_year = year\n",
    "            \n",
    "            if year != current_year:\n",
    "                # Save the vectors for the previous year\n",
    "                if storage == 'csv':\n",
    "                    with open(f'../data/vectors/{current_year}_vectors.csv', 'w', encoding='utf-8', newline='') as writer:\n",
    "                        csv_writer = csv.writer(writer, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                        csv_writer.writerow([\"PaperID\"] + list(range(0, 768)))\n",
    "                        csv_writer.writerows(current_year_vectors)\n",
    "                elif storage == 'numpy':\n",
    "                    np.save(f'../data/vectors/{current_year}_vectors.npy', np.array(current_year_vectors))\n",
    "                \n",
    "                # Reset for the new year\n",
    "                current_year_vectors = []\n",
    "                current_year = year\n",
    "            \n",
    "            text = line[2] + line[3]\n",
    "            vector = embeddings.get_embedding(text, tokenizer, model)\n",
    "            current_year_vectors.append([line[0]] + list(vector))  # Add PaperID at the beginning\n",
    "\n",
    "        # Save vectors for the last year after the loop\n",
    "        if storage == 'csv':\n",
    "            with open(f'../data/vectors/{current_year}_vectors.csv', 'w', encoding='utf-8', newline='') as writer:\n",
    "                csv_writer = csv.writer(writer, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                csv_writer.writerow([\"PaperID\"] + list(range(0, 768)))\n",
    "                csv_writer.writerows(current_year_vectors)\n",
    "        elif storage == 'numpy':\n",
    "            np.save(f'../data/vectors/{current_year}_vectors.npy', np.array(current_year_vectors))            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
