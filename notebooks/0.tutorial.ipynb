{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39f9a609-699c-438f-ad02-af1176f316a7",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook shows how to implement novelty and impact metrics calculation for scientific papers.\n",
    "The notebook is a showcase for measuring novelty for a custom set of papers downloaded from OpenAlex.\n",
    "Novelty and impact is measured restricted to this subset of papers.\n",
    "\n",
    "The following steps are undertaken:\n",
    "1. **Data Collection**: Data is downloaded from OpenAlex using the API\n",
    "2. **Preprocessing**: Title and abstract of the subset of papers are processed\n",
    "3. **Text Embedding**: Title and abstract (non processed) of the subset of papers are transformed into 768-size vectors using SPECTER\n",
    "4. **Semantic Distance**: For each paper the average and maximum distance are calculated using the embedding vectors\n",
    "5. **New words**: For each paper the number of new words and their reuse in future papers are identified\n",
    "6. **New phrases**: For each paper the number of new noun phrases and their reuse in future papers are identified\n",
    "7. **New word combinations**: For each paper the number of new word combinations and their reuse in future papers are identified\n",
    "8. **New phrase combinations**: For each paper the number of new phrase combinations and their reuse in future papers are identified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48e456a-8a97-4607-9cfd-a03750ad3944",
   "metadata": {},
   "source": [
    "## Initial importings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afc63b2-4f27-4b1c-abe2-869352e0f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../science_novelty/')\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import preprocessing\n",
    "from tqdm.notebook import tqdm\n",
    "import csv\n",
    "\n",
    "## Increase the max size of a line reading, otherwise an error is raised\n",
    "maxInt = sys.maxsize\n",
    "\n",
    "while True:\n",
    "    # decrease the maxInt value by factor 10 \n",
    "    # as long as the OverflowError occurs.\n",
    "\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "\n",
    "import embeddings\n",
    "import new_ngram\n",
    "import new_ngram_comb\n",
    "import similarity_calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60844f82-99d3-4de0-834b-373c09b24c08",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "In this phase all papers containing the words \"natural language processing\" and \"novelty\" are downloaded from OpenAlex.\n",
    "\n",
    "The papers are downloaded in chunks of 100 papers and stored in a file tab-separated format called `papers_raw.csv`.\n",
    "The date of publication for each paper is important. Also, the papers must be ordered by publication date.\n",
    "\n",
    "The resulting file contains four columns: *PaperID*, *Date*, *Title* and *Abstract*.\n",
    "\n",
    "See the notebook [`1.data-collection.ipynb`](https://github.com/nicolamelluso/science-novelty/blob/main/notebooks/1.data-collection.ipynb) for more detailed information on this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8191e6f-c962-4f21-9360-4e718d7d7759",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OpenAlex API URL\n",
    "url = \"https://api.openalex.org/works\"\n",
    "\n",
    "# This is an example query\n",
    "query = '(natural language processing) & novelty'\n",
    "\n",
    "# Define the initial page and per page variables\n",
    "page = 1\n",
    "per_page = 100 \n",
    "papers = []\n",
    "\n",
    "params = {'search': query, 'filter': 'type:article'}\n",
    "\n",
    "# Send a GET request to the API\n",
    "response = requests.get(url, params=params)\n",
    "count = response.json()['meta']['count']\n",
    "total_pages = round(count/per_page) + 1\n",
    "\n",
    "print('Total papers: %d'%(count))\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "print('Start querying...')\n",
    "# Loop through all pages (+1 to get the last page)\n",
    "for page in tqdm(range(1,total_pages + 1)):\n",
    "    \n",
    "    # Get the cursor for the first page\n",
    "    if page == 1:\n",
    "        cursor = '*'\n",
    "\n",
    "    params = {\n",
    "        'search': query,\n",
    "        'sort': 'publication_date',\n",
    "        'per-page': per_page,\n",
    "        'filter': 'type:article',\n",
    "        'cursor' : cursor\n",
    "    }\n",
    "\n",
    "    # Send a GET request to the API\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    # If the request is successful\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "\n",
    "        # Get the data\n",
    "        results = data.get('results',[])\n",
    "        \n",
    "        # Select the information need from these publications\n",
    "        papers.extend([((res['id'].split('/')[-1].replace('W','')),\n",
    "                        res['publication_date'],\n",
    "                        res['title'],\n",
    "                        preprocessing.plain_text_from_inverted(res['abstract_inverted_index'])) \n",
    "                               for res in results])\n",
    "        \n",
    "        # Get the next cursor for the pagination\n",
    "        cursor = response.json()['meta']['next_cursor']\n",
    "\n",
    "        # Respect the API rate limit\n",
    "        time.sleep(1)\n",
    "        \n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}.\")\n",
    "        break\n",
    "    \n",
    "print('Creating the dataframe...')\n",
    "papers = pd.DataFrame(papers, columns = ['PaperID','Date','Title','Abstract'])\n",
    "\n",
    "print('Drop missing papers with missing title and abstract.')\n",
    "papers = papers.dropna(subset = ['Title','Abstract'], how = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c71a91-a3eb-47e9-8724-24d4703be084",
   "metadata": {},
   "source": [
    "## 2. Preprocessing \n",
    "\n",
    "In this phase, the subset of papers downloaded from OpenAlex is processed following the procedure in Arts et al. (2023).\n",
    "The processing phase consists of creating three files containing respectively the words, bigrams and trigrams.\n",
    "The preprocessing is done on-the-fly, meaning that papers are read one-by-one to not overload the memory.\n",
    "\n",
    "Three files are generated from this process and put in the `data/processed/` directory:\n",
    "\n",
    "- `papers_words.csv`: one row and three columns for each paper. Columns: *PaperID*, *Words_Title* and *Words_Abstract*.\n",
    "- `papers_phrases.csv`: one row and three columns for each paper. Columns: *PaperID*, *Phrases_Title* and *Phrases_Abstract*.\n",
    "\n",
    "See the notebook [`2.preprocessing.ipynb`](https://github.com/nicolamelluso/science-novelty/blob/main/notebooks/2.preprocessing.ipynb) for more detailed information on this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666cc5e1-5bb1-4378-a920-bb258fe6a564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Get the number of papers to process...')\n",
    "with open('../data/raw/papers_raw.csv', 'r', encoding = 'utf-8') as file:\n",
    "    line_count = sum(1 for line in file)\n",
    "\n",
    "# Subtract 1 for the header if the CSV has a header\n",
    "total_papers = line_count - 1\n",
    "\n",
    "print('Preparing for writing...')\n",
    "words_writer = open('../data/processed/papers_words.csv', mode = 'w', encoding = 'utf-8')\n",
    "words_writer.write('PaperID,Year,Words_Title,Words_Abstract\\n') # write the first line for the headers\n",
    "\n",
    "phrases_writer = open('../data/processed/papers_phrases.csv', mode = 'w', encoding = 'utf-8')\n",
    "phrases_writer.write('PaperID,Year,Phrases_Title,Phrases_Abstract\\n') # write the first line for the headers\n",
    "\n",
    "print('Processing...')\n",
    "with open('../data/raw/papers_raw.csv', mode = 'r', encoding='utf-8') as reader:\n",
    "    csv_reader = csv.reader(reader, delimiter='\\t', quotechar='\"')\n",
    "    \n",
    "    # Skip header\n",
    "    next(csv_reader)\n",
    "\n",
    "    for line in tqdm(csv_reader, total = total_papers):\n",
    "        \n",
    "        paperID, date, title, abstract = line # add the PaperID\n",
    "\n",
    "        year = date.split('-')[0]\n",
    "\n",
    "        title_words = preprocessing.process_text(title, 'words')\n",
    "        abstract_words = preprocessing.process_text(abstract, 'words')\n",
    "\n",
    "        title_phrases = preprocessing.process_text(title, 'phrases')\n",
    "        abstract_phrases = preprocessing.process_text(abstract, 'phrases')    \n",
    "            \n",
    "        words_writer.write(f'{paperID},{year},{title_words},{abstract_words}\\n')\n",
    "        phrases_writer.write(f'{paperID},{year},{title_phrases},{abstract_phrases}\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07736c4-ed86-486c-bb31-00da9df7dd0d",
   "metadata": {},
   "source": [
    "## 3. Text Embeddings \n",
    "\n",
    "In this phase, the subset of papers downloaded from OpenAlex is transformed into 768-size vectors using SPECTER.\n",
    "A python module called `embedding_generator` is imported to perform the this process.\n",
    "\n",
    "In this phase, a new file for each year is created, storing the embeddings in either CSV format depending in the directory `data/vectors/`\n",
    "\n",
    "See the notebook [`3.text-embeddings.ipynb`](https://github.com/nicolamelluso/science-novelty/blob/main/notebooks/3.text-embeddings.ipynb) for more detailed information on this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f0d778-03ad-434a-94b4-241f19c59afa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import embedding_generator as eg\n",
    "\n",
    "# Set your directories and parameters\n",
    "input_file = '../data/raw/papers_raw.csv'\n",
    "output_dir = '../data/vectors/'\n",
    "storage_method = 'csv'  # or 'numpy'\n",
    "chunk_size = 50\n",
    "\n",
    "# Call the function to process embeddings\n",
    "eg.process_embeddings(input_file, output_dir, storage=storage_method, chunk_size=chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e98eef-dda6-4609-b277-e54cb3c5543c",
   "metadata": {},
   "source": [
    "## 4. Semantic distance \n",
    "\n",
    "In this phase, we calculate the cosine distance between the text embeddings to measure the similarity between different papers. We utilize the `similarity_calculator`  module to efficiently compute these distances.\n",
    "\n",
    "The similarities are stored in a file called `papers_cosine.csv` in the directory `data/metrics`.\n",
    "\n",
    "The file contains for each paper one row and three columns: *PaperID*,*max_similarity* and *avg_similarity*.\n",
    "\n",
    "See the notebook [`4.cosine-distance.ipynb`](https://github.com/nicolamelluso/science-novelty/blob/main/notebooks/3.text-embeddings.ipynb) for more detailed information on this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe899f58-9af3-4703-8e3a-6a34b83648bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import similarity_calculator as sc\n",
    "\n",
    "# Set your directories\n",
    "input_dir = '../data/vectors/'\n",
    "output_dir = '../data/metrics/'\n",
    "\n",
    "# Get the years of the embeddings\n",
    "years = sorted([int(f.split('_')[0]) for f in os.listdir(input_dir) if 'check' not in f])\n",
    "\n",
    "# Set the range of years you want to process\n",
    "start_year = min(years)\n",
    "end_year = max(years) + 1\n",
    "\n",
    "# Call the function to calculate similarities\n",
    "sc.calculate_similarities(start_year, end_year, input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132465ed-edf1-4659-ad23-a0f9a640d869",
   "metadata": {},
   "source": [
    "## 5. New words\n",
    "\n",
    "In this phase, we identify the new words introduced by each paper. \n",
    "For each new word, the ID is the first paper is identified and the number of subsequent papers the re-use the word are counted.\n",
    "A baseline of words is defined. Words that appear in the baseline are not considered as new words. \n",
    "\n",
    "### Methodology:\n",
    "- **Establishing a Baseline**: A list of words serves as our baseline. Any word that belong to the baseline is not classified as a 'new word'\n",
    "\n",
    "- **Data Processing and Comparison**: The script code by importing data from a designated CSV file. Each word from the dataset is compared against the baseline to determine its novelty. The frequency of each new word, post its introduction, is counted.\n",
    "\n",
    "- **Utilizing the new_ngram Module**: To streamline and optimize the process of identifying and counting new words, we import the `new_ngram` module.\n",
    "\n",
    "### Prerequisites:\n",
    "- **Baseline Year**: A specific year must be defined as the terminal year for the baseline. All papers published prior to this year contribute to the baseline. Consequently, words from these papers are not considered as new words.\n",
    "\n",
    "- **Directory Definitions**:\n",
    "\n",
    "    - A directory containing the processed papers.\n",
    "    - A directory storing the publication year of each paper.\n",
    "\n",
    "### Assumptions:\n",
    "To ensure the seamless execution of the code, we operate under the following assumptions:\n",
    "\n",
    "- The dataset of papers is chronologically ordered based on their publication dates, with the most recent papers listed last.\n",
    "- The publication year of each paper is located in the second column of the second specified file (the file containing the year information).\n",
    "\n",
    "### Output:\n",
    "\n",
    "The findings from this phase are cataloged in a file named `new_words.csv`, located in the `data/metrics` directory. This file is structured with each row representing a new word and three columns detailing the word, the PaperID of the paper that introduced it, and the reuse count, indicating the frequency of its subsequent usage.\n",
    "\n",
    "See the notebook [`5.new-word.ipynb`](https://github.com/nicolamelluso/science-novelty/blob/main/notebooks/5.new-word.ipynb) for more detailed information on this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d9f18d-01b2-4c03-a6d3-e4f9f914ed58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import new_ngram\n",
    "\n",
    "# define the last year of the baseline and the directory of the dates and the processed file\n",
    "new_words_calculator = new_ngram.calculate_new_ngrams(2000, '../data/raw/papers_raw.csv', '../data/processed/papers_words.csv')\n",
    "new_words = new_words_calculator()\n",
    "\n",
    "new_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c066ce-5d5f-45f3-824d-63c4c8bdd219",
   "metadata": {},
   "source": [
    "## 6. New phrases\n",
    "\n",
    "In this phase, we identify the new phrases introduced by each paper. \n",
    "For each new phrase, the ID is the first paper is identified and the number of subsequent papers the re-use the phrase are counted.\n",
    "A baseline of phrases is defined. Phrases that appear in the baseline are not considered as new bigrams. \n",
    "\n",
    "The procedure is the same as for new words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2761c953-b3e9-415e-910e-1131fe523ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import new_ngram\n",
    "\n",
    "# define the last year of the baseline and the directory of the dates and the processed file\n",
    "new_phrases_calculator = new_ngram.calculate_new_ngrams(2000, '../data/raw/papers_raw.csv', '../data/processed/papers_phrases.csv')\n",
    "new_phrases = new_phrases_calculator()\n",
    "\n",
    "new_phrases_calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ac49d7-57bb-4ba0-bfc6-bfa936eb3a85",
   "metadata": {},
   "source": [
    "## 7. New word comb\n",
    "\n",
    "In this phase, we identify the new word combinations introduced by each paper. \n",
    "For each new word combination, the ID is the first paper is identified and the number of subsequent papers the re-use the word combination are counted.\n",
    "A baseline of word combinations is defined. Word combinations that appear in the baseline are not considered as new word combinations. \n",
    "\n",
    "The procedure is the same as for new words.\n",
    "\n",
    "To streamline and optimize the process of identifying and counting new words, we import the `new_ngram_comb` module.\n",
    "\n",
    "See the notebook [`8.new-word-comb.ipynb`](https://github.com/nicolamelluso/science-novelty/blob/main/notebooks/8.new-word-comb.ipynb) for more detailed information on this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc58b496-4b54-4c18-a042-1811680bf935",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import new_ngram_comb\n",
    "\n",
    "# define the last year of the baseline and the directory of the dates and the processed file\n",
    "new_word_combs_calculator = new_ngram_comb.calculate_new_ngram_combs(2000, '../data/raw/papers_raw.csv', '../data/processed/papers_words.csv')\n",
    "new_word_combs = new_word_combs_calculator()\n",
    "\n",
    "new_word_combs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103c20f0-3401-4de7-87ce-6acecbe92ffa",
   "metadata": {},
   "source": [
    "## 7. New phrase comb\n",
    "\n",
    "In this phase, we identify the new phrase combinations introduced by each paper. \n",
    "For each new phrase combination, the ID is the first paper is identified and the number of subsequent papers the re-use the phrase combination are counted.\n",
    "A baseline of phrase combinations is defined. Phrase combinations that appear in the baseline are not considered as new phrase combinations. \n",
    "\n",
    "The procedure is the same as for new phrases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b335f-1d03-4930-b7ad-cf2b81fc2879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import new_ngram_comb\n",
    "\n",
    "# define the last year of the baseline and the directory of the dates and the processed file\n",
    "new_phrase_combs_calculator = new_ngram_comb.calculate_new_ngram_combs(2000, '../data/raw/papers_raw.csv', '../data/processed/papers_phrases.csv')\n",
    "new_phrase_combs = new_word_combs_calculator()\n",
    "\n",
    "new_phrase_combs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
